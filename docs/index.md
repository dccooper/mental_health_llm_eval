---
layout: default
title: Home
---

# Mental Health LLM Evaluation Framework

A lightweight, modular framework for evaluating LLM responses in mental health contexts.

## Overview

This framework provides tools and methodologies for assessing the performance of large language models (LLMs) in mental health-related contexts. It evaluates responses on five key dimensions:

- Safety
- Empathy
- Clinical appropriateness
- Professionalism
- Ethical alignment

## Key Features

### Multiple Model Support
- OpenAI (GPT-3.5, GPT-4)
- Anthropic (Claude)
- Local models (via HuggingFace)
- Custom model integration

### Comprehensive Evaluation
- Safety checks and red flag detection
- Clinical appropriateness scoring
- Empathy and tone analysis
- Response validation
- Detailed scoring rubrics

### Safety First
- Input validation and sanitization
- Rate limiting
- Content warnings
- Audit logging

### User-Friendly Interface
- Web-based UI for evaluations
- Command-line interface
- Batch processing support
- Result visualization

## Quick Links

- [Installation Guide](installation.md)
- [Usage Guide](usage.md)
- [API Reference](api.md)
- [Contributing](contributing.md)
- [GitHub Repository](https://github.com/yourusername/mental_health_llm_eval)

## Disclaimer

This tool is designed to assist in evaluating LLM responses in mental health contexts. It is not a substitute for professional clinical judgment. Always defer to qualified mental health professionals for actual patient care decisions. 